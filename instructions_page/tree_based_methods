## Tree-based methods

# Preparations
 - Load libraries: tree, randomForest, ISLR
 - Explore dataset carseats

# Exercise
Now we will attempt to predict Sales using regression trees and related approaches, treating the response as a quantitative variable.
(a) Split the data set into a training set and a test set.
(b) Fit a normal regression tree to the training set. Plot the tree, and interpret the results. What test MSE do you obtain?
(c) Use cross-validation in order to determine the optimal level of tree complexity. Does pruning the tree improve the test MSE?
(d) Use the bagging approach in order to analyze this data. What test MSE do you obtain? Use the importance() function to determine which variables are most important.
(e) Use random forests to analyze this data. What test MSE do you obtain? Use the importance() function to determine which variables are most important.

# Hints
(a) # Remember to split your data in training and test sets, for this you can use the following function:
    train_indices = sample(...)
    # Sample requires a vector or an integer, an integer is easily obtained by applying the nrow() function to your dataframe.
(b) library(tree)
    var_name = tree('response_variable' ~ ., data = 'dataset', subset = 'train_indices')
(c) # Tree has a built-in cross-validation function, lucky for us!
    # As default cross validation has a K of 10. You can play with this
    cross_validated_tree = cv.tree('your tree variables')
(d) # For bagging we can use the randomForest package, with an adjusted variable of 'mtry' which indicates the number of features used.
    # Remember, bagging samples from all data with replacement and uses all features.
    car.forest = randomForest('your formula', data='...', subset='...', mtry='...', ntree='...')

(e) # This one should be easy, remember the difference between bagging and random forest. Use the random forest library.
